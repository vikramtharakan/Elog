{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(s,e):\n",
    "    '''\n",
    "    --- Imports data from Elog and stores it in a workable format ---\n",
    "    INPUT\n",
    "        s: start time as unix timestamp\n",
    "        e: end time as unix time stamp\n",
    "    RETURN\n",
    "        df: dataframe of uncleaned data between selected time range\n",
    "    '''\n",
    "    \n",
    "    # api-endpoint \n",
    "    URL = \"https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_elog_display_json.php\"\n",
    "\n",
    "    PARAMS = {'logbook': 'MCC', 'start': s, 'end': e} \n",
    "\n",
    "    # sending get request and saving the response as response object \n",
    "    r = requests.get(url = URL, params = PARAMS) \n",
    "\n",
    "    # extracting data in json format \n",
    "    data = r.json()\n",
    "\n",
    "    # Turning list of json objects into dataframe\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    --- Cleans data frame ---\n",
    "    INPUT\n",
    "        df: dataframe (not cleaned)\n",
    "    RETURN\n",
    "        df: dataframe (cleaned)\n",
    "    '''\n",
    "    # Checks to make sure there are even entries with a tag in the specified month\n",
    "    if 'tag' not in df.columns:\n",
    "        return 0\n",
    "    \n",
    "    # Dropping rows without any tags (these rows are useless for us)\n",
    "    df = df[df.tag.notnull() == True]\n",
    "    \n",
    "    # Dropping useless columns\n",
    "    important_cols = {'title', 'text', 'elogid', 'tag', 'superseded_by'}\n",
    "    list1 = df.columns.tolist()\n",
    "    list1 = [ele for ele in list1 if ele not in important_cols]\n",
    "    for column in df.columns.tolist():\n",
    "        if column in list1:\n",
    "            df = df.drop(column,axis = 1)\n",
    "\n",
    "    # Dropping all columns where superceded_by is not null to essentially drop duplicates. Then drop superceded_by column\n",
    "    df = df[df['superseded_by'].isnull() == True]\n",
    "    df = df.drop(['superseded_by'],axis = 1)\n",
    "    df = df.drop_duplicates(subset =\"elogid\", keep = 'first')\n",
    "    \n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data_2011():\n",
    "    '''\n",
    "    --- Builds one giant dataframe by concating data frames together one month at a time ---\n",
    "    RETURN\n",
    "        df: Cleaned dataframe of tagged entries from April 2007 - December 2011.    \n",
    "    '''\n",
    "    year_list = [2007,2008,2009,2010,2011]\n",
    "    month_list = list(range(1,13))\n",
    "    df = pd.DataFrame(columns=['elogid', 'title', 'text', 'tag'])\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            if (year == 2007 and month < 4):\n",
    "                continue\n",
    "            elif (month == 12):\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year+1, 1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp)\n",
    "            else:\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year, month+1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp)\n",
    "            \n",
    "            # Checks to make sure cleaned dataframe actually has any tags\n",
    "            if isinstance(df_temp, pd.DataFrame) == True:\n",
    "                print(str(month)+'/'+str(year) + ':  ' + str(df_temp.shape[0]))\n",
    "                df = pd.concat([df,df_temp], ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the data into sql database\n",
    "def save_data(df, database_filename):\n",
    "    engine = create_engine('sqlite:///'+database_filename+'.db')\n",
    "    df.to_sql(database_filename, engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Will go through all the necessary steps to extract the data from the elog, clean it, and save the data\n",
    "    in an SQL database\n",
    "    '''\n",
    "    df = join_data_2011()\n",
    "    save_data(df,'elog_data_2011')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this will save the data that we want to collect\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
