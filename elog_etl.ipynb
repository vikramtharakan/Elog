{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elog Tagging\n",
    "\n",
    "The goal is to try and tag elog entries with the correct tag. In order to do this we need to:\n",
    "* Scrape the data logbook\n",
    "* Clean data (drop duplicates, only keep data with tags, make sure text body is in proper format..)\n",
    "* Save data in easy to access way for NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(s,e):\n",
    "    '''\n",
    "    --- Imports data from Elog and stores it in a workable format ---\n",
    "    INPUT\n",
    "        s: start time as unix timestamp\n",
    "        e: end time as unix time stamp\n",
    "    RETURN\n",
    "        df: dataframe of uncleaned data between selected time range\n",
    "    '''\n",
    "    \n",
    "    # api-endpoint \n",
    "    URL = \"https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_elog_display_json.php\"\n",
    "\n",
    "    PARAMS = {'logbook': 'MCC', 'start': s, 'end': e} \n",
    "\n",
    "    # sending get request and saving the response as response object \n",
    "    r = requests.get(url = URL, params = PARAMS) \n",
    "\n",
    "    # extracting data in json format \n",
    "    data = r.json()\n",
    "\n",
    "    # Turning list of json objects into dataframe\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    --- Cleans data frame ---\n",
    "    INPUT\n",
    "        df: dataframe (not cleaned)\n",
    "    RETURN\n",
    "        df: dataframe (cleaned)\n",
    "    '''\n",
    "    # Checks to make sure there are even entries with a tag in the specified month\n",
    "    if 'tag' not in df.columns:\n",
    "        return 0\n",
    "    \n",
    "    # Dropping rows without any tags (these rows are useless for us)\n",
    "    df = df[df.tag.notnull() == True]\n",
    "    \n",
    "    # Dropping useless columns\n",
    "    important_cols = {'title', 'text', 'elogid', 'tag', 'superseded_by'}\n",
    "    list1 = df.columns.tolist()\n",
    "    list1 = [ele for ele in list1 if ele not in important_cols]\n",
    "    for column in df.columns.tolist():\n",
    "        if column in list1:\n",
    "            df = df.drop(column,axis = 1)\n",
    "\n",
    "    # Dropping all columns where superceded_by is not null to essentially drop duplicates. Then drop superceded_by column\n",
    "    df = df[df['superseded_by'].isnull() == True]\n",
    "    df = df.drop(['superseded_by'],axis = 1)\n",
    "    df = df.drop_duplicates(subset =\"elogid\", keep = 'first')\n",
    "    \n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_data_2011():\n",
    "    '''\n",
    "    --- Builds one giant dataframe by concating data frames together one month at a time ---\n",
    "    RETURN\n",
    "        df: Cleaned dataframe of tagged entries from April 2007 - December 2011.    \n",
    "    '''\n",
    "    year_list = [2007,2008,2009,2010,2011]\n",
    "    month_list = list(range(1,13))\n",
    "    df = pd.DataFrame(columns=['elogid', 'title', 'text', 'tag'])\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            if (year == 2007 and month < 4):\n",
    "                continue\n",
    "            elif (month == 12):\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year+1, 1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp)\n",
    "            else:\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year, month+1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp)\n",
    "            \n",
    "            # Checks to make sure cleaned dataframe actually has any tags\n",
    "            if isinstance(df_temp, pd.DataFrame) == True:\n",
    "                print(str(month)+'/'+str(year) + ':  ' + str(df_temp.shape[0]))\n",
    "                df = pd.concat([df,df_temp], ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the data as .db file\n",
    "def save_data(df, database_filename):\n",
    "    engine = create_engine('sqlite:///'+database_filename+'.db')\n",
    "    df.to_sql(database_filename, engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the main function that will use the actually compile the data and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Will go through all the necessary steps to extract the data from the elog, clean it, and save the data\n",
    "    in an SQL database\n",
    "    '''\n",
    "    df = join_data_2011()\n",
    "    save_data(df,'elog_data_2011')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/2007:  2\n",
      "5/2007:  3\n",
      "8/2007:  2\n",
      "12/2007:  3\n",
      "1/2008:  11\n",
      "2/2008:  11\n",
      "3/2008:  15\n",
      "4/2008:  5\n",
      "6/2008:  3\n",
      "7/2008:  1\n",
      "8/2008:  5\n",
      "11/2008:  1\n",
      "12/2008:  1\n",
      "1/2009:  3\n",
      "2/2009:  10\n",
      "4/2009:  2\n",
      "5/2009:  19\n",
      "6/2009:  51\n",
      "7/2009:  30\n",
      "8/2009:  39\n",
      "9/2009:  25\n",
      "10/2009:  18\n",
      "11/2009:  32\n",
      "12/2009:  16\n",
      "4/2010:  28\n",
      "5/2010:  22\n",
      "6/2010:  20\n",
      "7/2010:  15\n",
      "8/2010:  17\n",
      "9/2010:  12\n",
      "10/2010:  22\n",
      "11/2010:  9\n",
      "12/2010:  12\n",
      "1/2011:  21\n",
      "2/2011:  13\n",
      "3/2011:  5\n",
      "4/2011:  4\n",
      "5/2011:  2\n",
      "6/2011:  14\n",
      "7/2011:  782\n",
      "8/2011:  1616\n",
      "9/2011:  1246\n",
      "10/2011:  1240\n",
      "11/2011:  1043\n",
      "12/2011:  462\n"
     ]
    }
   ],
   "source": [
    "# Running this will save the data that we want to collect\n",
    "df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to be able to deal with:\n",
    "* Tables\n",
    "* special characters (new line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
