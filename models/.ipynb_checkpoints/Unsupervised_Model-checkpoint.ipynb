{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251193, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting 'elog_all_data' table from 'elog_data.db' database\n",
    "conn = sqlite3.connect(r'/Users/vikram/Projects/Elog/data/elog_data.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT * FROM elog_all_data')\n",
    "df = pd.DataFrame(c.fetchall(), columns=['elogid', 'tag', 'text', 'title', 'title_and_text'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We want to implement a form of unsupervised learning that can <i>ideally</i> cluster these entries into 2 groups (LCLS and FACET). This will allow us to train our data on a much larger sample size across a much bigger time period, which will hopefully make our model more robust\n",
    "\n",
    "### Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat tokenizer function\n",
    "def tokenize(x):\n",
    "    \n",
    "    # Generating list of stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Separate sentance into individual words\n",
    "    no_punctuation_x = re.sub(r\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    word_token = word_tokenize(no_punctuation_x)\n",
    "    \n",
    "    # Lemmatizing each word and added cleaned words to clean_words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_words = []\n",
    "    for word in word_token:\n",
    "        clean_words.append(lemmatizer.lemmatize(word.lower().strip()))\n",
    "\n",
    "    # Return lematized words that are indeed words and are not in stopwords list\n",
    "    final_token = [w for w in clean_words if w not in stop_words]\n",
    "    return final_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Method\n",
    "\n",
    "Let's practice on a smaller subset of the data fram at first (let's say 100 random entries) and only test it on the title. We can then move onto incorporating the text after we get a working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elogid</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>title_and_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>248309</th>\n",
       "      <td>966617</td>\n",
       "      <td>LCLS</td>\n",
       "      <td>They are going to test gas jet monitor with be...</td>\n",
       "      <td>XLEAPers taking a break, putting in slotted fo...</td>\n",
       "      <td>XLEAPers taking a break, putting in slotted fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81831</th>\n",
       "      <td>426426</td>\n",
       "      <td>None</td>\n",
       "      <td>Reset locally, then IPL brought it back.</td>\n",
       "      <td>MC00 offline.</td>\n",
       "      <td>MC00 offline. Reset locally, then IPL brought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226727</th>\n",
       "      <td>904412</td>\n",
       "      <td>LCLS</td>\n",
       "      <td>This is behind the S6 stopper but evidently th...</td>\n",
       "      <td>* Re: MPS fault for MEC BXL VGC 01 Gate Valve ...</td>\n",
       "      <td>* Re: MPS fault for MEC BXL VGC 01 Gate Valve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238180</th>\n",
       "      <td>939703</td>\n",
       "      <td>LCLS</td>\n",
       "      <td></td>\n",
       "      <td>27-4 to standby for PEM job.</td>\n",
       "      <td>27-4 to standby for PEM job.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126328</th>\n",
       "      <td>625259</td>\n",
       "      <td>LCLS</td>\n",
       "      <td></td>\n",
       "      <td>James Bong will be here in 1 hour.</td>\n",
       "      <td>James Bong will be here in 1 hour.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        elogid   tag                                               text  \\\n",
       "248309  966617  LCLS  They are going to test gas jet monitor with be...   \n",
       "81831   426426  None           Reset locally, then IPL brought it back.   \n",
       "226727  904412  LCLS  This is behind the S6 stopper but evidently th...   \n",
       "238180  939703  LCLS                                                      \n",
       "126328  625259  LCLS                                                      \n",
       "\n",
       "                                                    title  \\\n",
       "248309  XLEAPers taking a break, putting in slotted fo...   \n",
       "81831                                       MC00 offline.   \n",
       "226727  * Re: MPS fault for MEC BXL VGC 01 Gate Valve ...   \n",
       "238180                       27-4 to standby for PEM job.   \n",
       "126328                 James Bong will be here in 1 hour.   \n",
       "\n",
       "                                           title_and_text  \n",
       "248309  XLEAPers taking a break, putting in slotted fo...  \n",
       "81831   MC00 offline. Reset locally, then IPL brought ...  \n",
       "226727  * Re: MPS fault for MEC BXL VGC 01 Gate Valve ...  \n",
       "238180                      27-4 to standby for PEM job.   \n",
       "126328                James Bong will be here in 1 hour.   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# practice with first 100 entries.\n",
    "df_abbr = df.sample(n = 100)\n",
    "df_abbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bag of words for our 100 log entry sample\n",
    "vectorizer = CountVectorizer(tokenizer = tokenize)       # Calls our tokenize function written above\n",
    "bag_of_words = vectorizer.fit_transform(df.title)        # Fitting just the title column for now\n",
    "bag_of_words.todense()                                   # Visual verification of bag of words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SVD down to 2 components for our bag_of_words\n",
    "svd = TruncatedSVD(n_components = 2)\n",
    "lsa = svd.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.254356</td>\n",
       "      <td>XPP requests +10 eV, vernier to 8 MeV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-0.000346</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>phase cavity signal shows much more jitter aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>went to shorter-pulse mode for SXR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.085495</td>\n",
       "      <td>* Fixed Re: * Thyratron might be dying Re: Bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.259056</td>\n",
       "      <td>-0.031412</td>\n",
       "      <td>DAY SHIFT SUMMARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.277085</td>\n",
       "      <td>* Re: Beam back on.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.222923</td>\n",
       "      <td>Beam is 25x28x25 in FACET. Users will be in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.127026</td>\n",
       "      <td>1.088427</td>\n",
       "      <td>Delivered to BaBar 7831 x10^30/cm^2 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.260219</td>\n",
       "      <td>No usable spares in L3 but we do still have th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.037894</td>\n",
       "      <td>CXI is ready for beam again.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic1    topic2                                               body\n",
       "90  0.001621  0.254356              XPP requests +10 eV, vernier to 8 MeV\n",
       "91 -0.000346  0.004098  phase cavity signal shows much more jitter aft...\n",
       "92  0.003137  0.009775                 went to shorter-pulse mode for SXR\n",
       "93  0.014408  0.085495  * Fixed Re: * Thyratron might be dying Re: Bea...\n",
       "94  1.259056 -0.031412                                  DAY SHIFT SUMMARY\n",
       "95  0.013057  0.277085                                * Re: Beam back on.\n",
       "96  0.013497  0.222923  Beam is 25x28x25 in FACET. Users will be in a ...\n",
       "97  0.127026  1.088427              Delivered to BaBar 7831 x10^30/cm^2 s\n",
       "98  0.001016  0.260219  No usable spares in L3 but we do still have th...\n",
       "99  0.011643  0.037894                       CXI is ready for beam again."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_encoded_df = pd.DataFrame(lsa, columns = ['topic1', 'topic2'])\n",
    "topic_encoded_df['body'] = df['title'].tolist()\n",
    "topic_encoded_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
