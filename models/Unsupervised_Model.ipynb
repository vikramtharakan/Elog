{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/vikram/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vikram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251193, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting 'elog_all_data' table from 'elog_data.db' database\n",
    "conn = sqlite3.connect(r'/Users/vikram/Projects/Elog/data/elog_data.db')\n",
    "c = conn.cursor()\n",
    "c.execute('SELECT * FROM elog_all_data')\n",
    "df = pd.DataFrame(c.fetchall(), columns=['elogid', 'tag', 'text', 'title', 'title_and_text'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We want to implement a form of unsupervised learning that can <i>ideally</i> cluster these entries into 2 groups (LCLS and FACET). This will allow us to train our data on a much larger sample size across a much bigger time period, which will hopefully make our model more robust\n",
    "\n",
    "### Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat tokenizer function\n",
    "def tokenize(x):\n",
    "    \n",
    "    # Generating list of stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Separate sentance into individual words\n",
    "    no_punctuation_x = re.sub(r\"[^a-zA-Z0-9]\",\" \", x)\n",
    "    word_token = word_tokenize(no_punctuation_x)\n",
    "    \n",
    "    # Lemmatizing each word and added cleaned words to clean_words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_words = []\n",
    "    for word in word_token:\n",
    "        clean_words.append(lemmatizer.lemmatize(word.lower().strip()))\n",
    "\n",
    "    # Return lematized words that are indeed words and are not in stopwords list\n",
    "    final_token = [w for w in clean_words if w not in stop_words]\n",
    "    return final_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Method\n",
    "\n",
    "Let's practice on a smaller subset of the data fram at first (let's say 100 random entries) and only test it on the title. We can then move onto incorporating the text after we get a working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elogid</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>title_and_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143742</th>\n",
       "      <td>672269</td>\n",
       "      <td>LCLS</td>\n",
       "      <td></td>\n",
       "      <td>BSY valves IV2-6 closed at Baker's request</td>\n",
       "      <td>BSY valves IV2-6 closed at Baker's request</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135342</th>\n",
       "      <td>651285</td>\n",
       "      <td>LCLS</td>\n",
       "      <td>There are a ton of plots in the LCLSlog - we i...</td>\n",
       "      <td>* Re: L1X amplitude two-state</td>\n",
       "      <td>* Re: L1X amplitude two-state There are a ton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211774</th>\n",
       "      <td>858993</td>\n",
       "      <td>LCLS</td>\n",
       "      <td></td>\n",
       "      <td>Bypassing A-line gauge PS3 to pump down with P...</td>\n",
       "      <td>Bypassing A-line gauge PS3 to pump down with P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207501</th>\n",
       "      <td>851132</td>\n",
       "      <td>LCLS</td>\n",
       "      <td>XTCAV will likely need to be rephased when we ...</td>\n",
       "      <td>PEM/AMRF working on XTCAV TWT, tightening PAD ...</td>\n",
       "      <td>PEM/AMRF working on XTCAV TWT, tightening PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144972</th>\n",
       "      <td>675854</td>\n",
       "      <td>LCLS</td>\n",
       "      <td>Pedro is alone tonight.\\nHe has a list from sw...</td>\n",
       "      <td>Touched base with PEM for tonight's plans.</td>\n",
       "      <td>Touched base with PEM for tonight's plans. Ped...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        elogid   tag                                               text  \\\n",
       "143742  672269  LCLS                                                      \n",
       "135342  651285  LCLS  There are a ton of plots in the LCLSlog - we i...   \n",
       "211774  858993  LCLS                                                      \n",
       "207501  851132  LCLS  XTCAV will likely need to be rephased when we ...   \n",
       "144972  675854  LCLS  Pedro is alone tonight.\\nHe has a list from sw...   \n",
       "\n",
       "                                                    title  \\\n",
       "143742         BSY valves IV2-6 closed at Baker's request   \n",
       "135342                      * Re: L1X amplitude two-state   \n",
       "211774  Bypassing A-line gauge PS3 to pump down with P...   \n",
       "207501  PEM/AMRF working on XTCAV TWT, tightening PAD ...   \n",
       "144972         Touched base with PEM for tonight's plans.   \n",
       "\n",
       "                                           title_and_text  \n",
       "143742        BSY valves IV2-6 closed at Baker's request   \n",
       "135342  * Re: L1X amplitude two-state There are a ton ...  \n",
       "211774  Bypassing A-line gauge PS3 to pump down with P...  \n",
       "207501  PEM/AMRF working on XTCAV TWT, tightening PAD ...  \n",
       "144972  Touched base with PEM for tonight's plans. Ped...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# practice with first 100 entries.\n",
    "df_abbr = df.sample(n = 100)\n",
    "df_abbr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a bag of words for our 100 log entry sample\n",
    "vectorizer = CountVectorizer(tokenizer = tokenize)       # Calls our tokenize function written above\n",
    "bag_of_words = vectorizer.fit_transform(df_abbr.title)   # Fitting just the title column for now\n",
    "bag_of_words.todense()                                   # Visual verification of bag of words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SVD down to 2 components for our bag_of_words\n",
    "svd = TruncatedSVD(n_components = 2)\n",
    "lsa = svd.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
