{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elog Tagging\n",
    "\n",
    "The goal is to try and tag elog entries with the correct tag. In order to do this we need to:\n",
    "* Scrape the data logbook\n",
    "* Clean data (drop duplicates, only keep data with tags, make sure text body is in proper format..)\n",
    "* Save data in easy to access way for NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(s,e):\n",
    "    '''\n",
    "    --- Imports data from Elog and stores it in a workable format ---\n",
    "    INPUT:\n",
    "        s  - start time as unix timestamp\n",
    "        e  - end time as unix time stamp\n",
    "    RETURN:\n",
    "        df - dataframe of uncleaned data between selected time range\n",
    "    '''\n",
    "    \n",
    "    # api-endpoint \n",
    "    URL = \"https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_elog_display_json.php\"\n",
    "\n",
    "    PARAMS = {'logbook': 'MCC', 'start': s, 'end': e} \n",
    "\n",
    "    # sending get request and saving the response as response object \n",
    "    r = requests.get(url = URL, params = PARAMS) \n",
    "\n",
    "    # extracting data in json format \n",
    "    data = r.json()\n",
    "\n",
    "    # Turning list of json objects into dataframe\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, only_tags = True):\n",
    "    '''\n",
    "    --- Cleans data frame ---\n",
    "    INPUT:\n",
    "        df        - dataframe (not cleaned)\n",
    "        only_tags - Boolean that determines if we are only keeping entries with tags or if we are keeping all entries\n",
    "    \n",
    "    RETURN:\n",
    "        df        - dataframe (cleaned)\n",
    "    '''\n",
    "    \n",
    "    # Checks to make sure there are even entries with a tag in the specified month (only for tagged data cleaning)\n",
    "    if only_tags == True:\n",
    "        if 'tag' not in df.columns:\n",
    "            return 0\n",
    "\n",
    "        # Dropping rows without any tags (these rows are useless for us)\n",
    "        df = df[df.tag.notnull() == True]\n",
    "    \n",
    "    # Dropping useless columns\n",
    "    important_cols = {'title', 'text', 'elogid', 'tag', 'superseded_by'}\n",
    "    list1 = df.columns.tolist()\n",
    "    list1 = [ele for ele in list1 if ele not in important_cols]\n",
    "    for column in df.columns.tolist():\n",
    "        if column in list1:\n",
    "            df = df.drop(column,axis = 1)\n",
    "\n",
    "    # Dropping all columns where superceded_by is not null to essentially drop duplicates. Then drop superceded_by column\n",
    "    df = df[df['superseded_by'].isnull() == True]\n",
    "    df = df.drop(['superseded_by'],axis = 1)\n",
    "    df = df.drop_duplicates(subset =\"elogid\", keep = 'first')\n",
    "\n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_all_data(only_tags = True):\n",
    "    '''\n",
    "    --- Builds one giant dataframe by concating data frames together one month at a time ---\n",
    "    INPUT:\n",
    "        only_tags   - Determines if we are only keeping entries with tags (True) or if we are keeping all entries (False)\n",
    "    \n",
    "    RETURN:\n",
    "        df          - Cleaned dataframe of either: \n",
    "                        ---> tagged entries from April 2007 - December 2011\n",
    "                        ---> all entries (tagged & not tagged) from 2007 through 2018\n",
    "    '''\n",
    "    year_list = list(range(2007,2019))\n",
    "    month_list = list(range(1,13))\n",
    "    df = pd.DataFrame(columns=['elogid', 'title', 'text', 'tag'])\n",
    "    for year in year_list:\n",
    "        for month in month_list:\n",
    "            if (year == 2007 and month < 4):\n",
    "                continue\n",
    "            elif (only_tags == True and year >= 2012):\n",
    "                break\n",
    "            elif (month == 12):\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year+1, 1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp, only_tags)\n",
    "            else:\n",
    "                s = datetime(year, month, 1, 0, 0).timestamp()\n",
    "                e = datetime(year, month+1, 1, 0, 0).timestamp()\n",
    "                df_temp = get_data(s,e)\n",
    "                df_temp = clean_data(df_temp, only_tags)\n",
    "            \n",
    "            # Checks to make sure cleaned dataframe actually has any tags\n",
    "            if isinstance(df_temp, pd.DataFrame) == True:\n",
    "                print(str(month)+'/'+str(year) + ':  ' + str(df_temp.shape[0]))\n",
    "                df = pd.concat([df,df_temp], ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the data as .db file\n",
    "def save_data(df, database_filename, only_tags = True, n = 10):\n",
    "    '''\n",
    "    Function to save the data to sqlite database to be loaded from later by the model\n",
    "    \n",
    "    INPUT: df                 - dataframe to be saved as .db file\n",
    "           database_filename  - filename for particular .db file\n",
    "           only tags          - True for tagged entries only, False for all data\n",
    "           n                  - Number of chuncks to divide large df into. Necessary for the all_data dataframe\n",
    "           \n",
    "    RETURN: \n",
    "           .db file(s) stored in the current directory to allow easy loading later\n",
    "    '''\n",
    "    \n",
    "    if only_tags == True:\n",
    "        engine = create_engine('sqlite:///'+database_filename+'.db')\n",
    "        df.to_sql(database_filename, engine, index=False)\n",
    "    \n",
    "    if only_tags == False:\n",
    "        df_big = np.array_split(df, n)\n",
    "        chunk_list = list(range(0,n))\n",
    "        for i in chunk_list:\n",
    "            engine = create_engine('sqlite:///'+database_filename+str(i)+'.db')\n",
    "            df_big[i].to_sql(database_filename, engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the main function that will use the actually compile the data and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Will go through all the necessary steps to extract the data from the elog, clean it, and save the data\n",
    "    in an SQL database\n",
    "    \n",
    "    ---Parameters--- Two parameters you may want to adjust\n",
    "    \n",
    "        only_tags   - If you want only tagged data (through 2011), set TRUE. If you want all data, set FALSE\n",
    "        names       - Current names for saving the cleaned Data: \n",
    "                       names[0] --> name for tagged data (when only_tags == True)\n",
    "                       names[1] --> name for all untagged data (when only_tags == False)\n",
    "                \n",
    "    RETURN: \n",
    "        df          - Returns DataFrame locally (used for testing). Main output is data stored in .db file(s)\n",
    "    '''\n",
    "    \n",
    "    # Set these variables prior to running main function. See main() documentation\n",
    "    only_tags = True\n",
    "    names = ['elog_data_2011', 'elog_all_data']\n",
    "\n",
    "    \n",
    "    # Extracts/Cleans Data\n",
    "    df = join_all_data(only_tags)\n",
    "    df['title_and_text'] = df['title'].str.cat(df['text'], sep =\" \")\n",
    "    \n",
    "    # Saves data to database\n",
    "    if only_tags == True:\n",
    "        try:\n",
    "            save_data(df,names[0])\n",
    "        except:\n",
    "            print('Already a file called ' + str(names[0]))\n",
    "    else:\n",
    "        try:\n",
    "            save_data(df,names[1],False,10)\n",
    "        except:\n",
    "            print('Already a file called ' + str(names[1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/2007:  3413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/2007:  2603\n",
      "6/2007:  4149\n",
      "7/2007:  4353\n",
      "8/2007:  3939\n",
      "9/2007:  584\n",
      "10/2007:  121\n",
      "11/2007:  422\n",
      "12/2007:  3000\n",
      "1/2008:  3721\n",
      "2/2008:  3447\n",
      "3/2008:  2844\n",
      "4/2008:  1486\n",
      "5/2008:  1607\n",
      "6/2008:  1537\n",
      "7/2008:  1439\n",
      "8/2008:  1211\n",
      "9/2008:  408\n",
      "10/2008:  613\n",
      "11/2008:  1609\n",
      "12/2008:  1410\n",
      "1/2009:  1832\n",
      "2/2009:  1999\n",
      "3/2009:  1172\n",
      "4/2009:  2326\n",
      "5/2009:  2137\n",
      "6/2009:  2206\n",
      "7/2009:  2144\n",
      "8/2009:  2491\n",
      "9/2009:  2258\n",
      "10/2009:  2120\n",
      "11/2009:  2033\n",
      "12/2009:  1320\n",
      "1/2010:  312\n",
      "2/2010:  198\n",
      "3/2010:  364\n",
      "4/2010:  2323\n",
      "5/2010:  2498\n",
      "6/2010:  2258\n",
      "7/2010:  2370\n",
      "8/2010:  1889\n",
      "9/2010:  1755\n",
      "10/2010:  2042\n",
      "11/2010:  2051\n",
      "12/2010:  1142\n",
      "1/2011:  2013\n",
      "2/2011:  1763\n",
      "3/2011:  739\n",
      "4/2011:  307\n",
      "5/2011:  1336\n",
      "6/2011:  2735\n",
      "7/2011:  2217\n",
      "8/2011:  2172\n",
      "9/2011:  1844\n",
      "10/2011:  1816\n",
      "11/2011:  1805\n",
      "12/2011:  941\n",
      "1/2012:  1768\n",
      "2/2012:  1778\n",
      "3/2012:  2531\n",
      "4/2012:  2339\n",
      "5/2012:  2710\n",
      "6/2012:  2320\n",
      "7/2012:  1733\n",
      "8/2012:  639\n",
      "9/2012:  868\n",
      "10/2012:  1803\n",
      "11/2012:  1694\n",
      "12/2012:  1123\n",
      "1/2013:  1654\n",
      "2/2013:  1845\n",
      "3/2013:  2486\n",
      "4/2013:  2701\n",
      "5/2013:  2435\n",
      "6/2013:  2388\n",
      "7/2013:  1810\n",
      "8/2013:  563\n",
      "9/2013:  1216\n",
      "10/2013:  2336\n",
      "11/2013:  2167\n",
      "12/2013:  1669\n",
      "1/2014:  1751\n",
      "2/2014:  2131\n",
      "3/2014:  2090\n",
      "4/2014:  1915\n",
      "5/2014:  2450\n",
      "6/2014:  2430\n",
      "7/2014:  1700\n",
      "8/2014:  488\n",
      "9/2014:  1322\n",
      "10/2014:  2341\n",
      "11/2014:  2323\n",
      "12/2014:  1421\n",
      "1/2015:  1806\n",
      "2/2015:  2179\n",
      "3/2015:  2261\n",
      "4/2015:  2246\n",
      "5/2015:  2592\n",
      "6/2015:  1829\n",
      "7/2015:  1814\n",
      "8/2015:  762\n",
      "9/2015:  1514\n",
      "10/2015:  2738\n",
      "11/2015:  2506\n",
      "12/2015:  1242\n",
      "1/2016:  2145\n",
      "2/2016:  2081\n",
      "3/2016:  2559\n",
      "4/2016:  1838\n",
      "5/2016:  1462\n",
      "6/2016:  1719\n",
      "7/2016:  1861\n",
      "8/2016:  1864\n",
      "9/2016:  1823\n",
      "10/2016:  1933\n",
      "11/2016:  1818\n",
      "12/2016:  1060\n",
      "1/2017:  110\n",
      "2/2017:  88\n",
      "3/2017:  138\n",
      "4/2017:  141\n",
      "5/2017:  2005\n",
      "6/2017:  2126\n",
      "7/2017:  1622\n",
      "8/2017:  1480\n",
      "9/2017:  1706\n",
      "10/2017:  1501\n",
      "11/2017:  1571\n",
      "12/2017:  984\n",
      "1/2018:  1558\n",
      "2/2018:  1650\n",
      "3/2018:  1551\n",
      "4/2018:  1804\n",
      "5/2018:  1784\n",
      "6/2018:  493\n",
      "7/2018:  267\n",
      "8/2018:  1732\n",
      "9/2018:  1981\n",
      "10/2018:  1936\n",
      "11/2018:  2241\n",
      "12/2018:  1290\n"
     ]
    }
   ],
   "source": [
    "# Running this will save the data that we want to collect\n",
    "df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to be able to deal with:\n",
    "* Tables\n",
    "* special characters (new line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
